{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df97234",
   "metadata": {},
   "source": [
    "# Sentiment Analysis â€”\n",
    "\n",
    "This notebook contains a complete end-to-end sentiment analysis pipeline (data inspection, cleaning, preprocessing, EDA, vectorization, modeling, evaluation, and saving artifacts). Copy & run cells in Jupyter. Adjust file paths if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Imports & Settings ===\n",
    "import os, re, warnings, joblib\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams.update({\"figure.max_open_warning\": 0, \"figure.figsize\": (8,4)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Paths & Input\n",
    "# -------------------------\n",
    "INPUT_PATH = \"/mnt/data/sentiment_reviews.csv\"  # change if needed\n",
    "CLEANED_CSV = \"/mnt/data/sentiment_reviews_cleaned.csv\"\n",
    "MODEL_DIR = \"/mnt/data/models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74efd103",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Cleaning\n",
    "# -------------------------\n",
    "df = df.drop_duplicates(subset=[\"review_text\"]).copy()\n",
    "df[\"review_text\"] = df[\"review_text\"].astype(str)\n",
    "df = df[df[\"review_text\"].str.strip().astype(bool)].copy()\n",
    "if \"sentiment\" in df.columns:\n",
    "    df = df[df[\"sentiment\"].notna()].copy()\n",
    "else:\n",
    "    raise ValueError(\"No 'sentiment' column found.\")\n",
    "\n",
    "if \"review_date\" in df.columns:\n",
    "    df[\"review_date\"] = pd.to_datetime(df[\"review_date\"], errors=\"coerce\")\n",
    "\n",
    "print(\"After cleaning shape:\", df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d9d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Preprocessing (text)\n",
    "# -------------------------\n",
    "try:\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "except Exception:\n",
    "    class _NaiveStemmer:\n",
    "        def stem(self, w):\n",
    "            for suf in (\"ing\",\"ly\",\"ed\",\"es\",\"s\"):\n",
    "                if w.endswith(suf) and len(w) > len(suf)+2:\n",
    "                    return w[:-len(suf)]\n",
    "            return w\n",
    "    stemmer = _NaiveStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    tokens = [stemmer.stem(tok) for tok in text.split() if len(tok) > 1]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"text_clean\"] = df[\"review_text\"].apply(preprocess_text)\n",
    "display(df[[\"review_text\", \"text_clean\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# EDA\n",
    "# -------------------------\n",
    "sent_counts = df[\"sentiment\"].value_counts()\n",
    "print(\"Sentiment distribution:\\n\", sent_counts)\n",
    "plt.bar(sent_counts.index, sent_counts.values); plt.title(\"Sentiment distribution\"); plt.show()\n",
    "\n",
    "if \"rating\" in df.columns:\n",
    "    rating_counts = df[\"rating\"].value_counts().sort_index()\n",
    "    print(\"Rating distribution:\\n\", rating_counts)\n",
    "    plt.bar(rating_counts.index.astype(str), rating_counts.values); plt.title(\"Rating distribution\"); plt.show()\n",
    "\n",
    "def top_n_words(series, n=20):\n",
    "    cnt = Counter()\n",
    "    for t in series:\n",
    "        cnt.update(t.split())\n",
    "    return pd.DataFrame(cnt.most_common(n), columns=[\"word\",\"count\"])\n",
    "\n",
    "print(\"\\nTop words overall:\")\n",
    "display(top_n_words(df[\"text_clean\"], n=20))\n",
    "\n",
    "for s in df[\"sentiment\"].unique():\n",
    "    print(f\"\\nTop words for sentiment = {s}:\")\n",
    "    top_df = top_n_words(df[df[\"sentiment\"]==s][\"text_clean\"], n=15)\n",
    "    display(top_df)\n",
    "    plt.bar(top_df[\"word\"], top_df[\"count\"]); plt.title(f\"Top words ({s})\"); plt.xticks(rotation=45); plt.tight_layout(); plt.show()\n",
    "\n",
    "if \"review_date\" in df.columns and df[\"review_date\"].notna().sum() > 0:\n",
    "    df_time = df.set_index(\"review_date\").resample(\"M\").size().rename(\"count\")\n",
    "    if len(df_time) > 1:\n",
    "        plt.plot(df_time.index, df_time.values); plt.title(\"Reviews over time (monthly)\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e667f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Vectorize & Encode\n",
    "# -------------------------\n",
    "X = df[\"text_clean\"].values.astype(str)\n",
    "y = df[\"sentiment\"].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "print(\"Label classes:\", le.classes_)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words=\"english\")\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "print(\"Vectorized shape:\", X_vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006deba1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Train/Test split & Models\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m      4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m----> 5\u001b[0m     X_vec, y_enc, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my_enc\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultinomialNB\u001b[39m\u001b[38;5;124m\"\u001b[39m: MultinomialNB(),\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogisticRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m: LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m),\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m\"\u001b[39m: RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_vec' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Train/Test split & Models\n",
    "# -------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Training: {name} ---\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    print(f\"{name} accuracy: {acc:.4f} f1_macro: {f1:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "    results[name] = {\"model\": model, \"accuracy\": acc, \"f1_macro\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb8e8ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Save best model & artifacts\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m best_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(results\u001b[38;5;241m.\u001b[39mkeys(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m k: results[k][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m best_model \u001b[38;5;241m=\u001b[39m results[best_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest model:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_macro:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results[best_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Save best model & artifacts\n",
    "# -------------------------\n",
    "best_name = max(results.keys(), key=lambda k: results[k][\"f1_macro\"])\n",
    "best_model = results[best_name][\"model\"]\n",
    "print(\"Best model:\", best_name, \"f1_macro:\", results[best_name][\"f1_macro\"])\n",
    "\n",
    "joblib.dump(best_model, os.path.join(MODEL_DIR, f\"{best_name}_model.pkl\"))\n",
    "joblib.dump(vectorizer, os.path.join(MODEL_DIR, \"tfidf_vectorizer.pkl\"))\n",
    "joblib.dump(le, os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
    "print(\"Saved artifacts to:\", MODEL_DIR)\n",
    "\n",
    "df.to_csv(CLEANED_CSV, index=False)\n",
    "print(\"Saved cleaned CSV to:\", CLEANED_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3e48a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Predict helper (examples)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_texts\u001b[39m(texts, model\u001b[38;5;241m=\u001b[39mbest_model, vectorizer_obj\u001b[38;5;241m=\u001b[39mvectorizer, label_enc\u001b[38;5;241m=\u001b[39mle):\n\u001b[0;32m      5\u001b[0m     texts_clean \u001b[38;5;241m=\u001b[39m [preprocess_text(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m      6\u001b[0m     Xv \u001b[38;5;241m=\u001b[39m vectorizer_obj\u001b[38;5;241m.\u001b[39mtransform(texts_clean)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Predict helper (examples)\n",
    "# -------------------------\n",
    "def predict_texts(texts, model=best_model, vectorizer_obj=vectorizer, label_enc=le):\n",
    "    texts_clean = [preprocess_text(t) for t in texts]\n",
    "    Xv = vectorizer_obj.transform(texts_clean)\n",
    "    preds = model.predict(Xv)\n",
    "    return label_enc.inverse_transform(preds)\n",
    "\n",
    "samples = [\n",
    "    \"This product is amazing, works exactly as described. Highly recommend!\",\n",
    "    \"Terrible service, product broke after 2 days. Do not buy.\",\n",
    "    \"It's okay, not great but not bad either.\"\n",
    "]\n",
    "print(\"Sample predictions:\")\n",
    "for s, p in zip(samples, predict_texts(samples)):\n",
    "    print(\"->\", s, \"=>\", p)\n",
    "\n",
    "# Example how to load outside:\n",
    "# loaded_vec = joblib.load('/mnt/data/models/tfidf_vectorizer.pkl')\n",
    "# loaded_model = joblib.load('/mnt/data/models/MultinomialNB_model.pkl')\n",
    "# loaded_le = joblib.load('/mnt/data/models/label_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3cde47",
   "metadata": {},
   "source": [
    "### Optional: Hyperparameter tuning\n",
    "Use GridSearchCV on LogisticRegression or RandomForest for better hyperparameters. Example code (commented) is available in the previous chat messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbdf862",
   "metadata": {},
   "source": [
    "## Extension: Advanced EDA, Cross-Validation, Hyperparameter Tuning, Error Analysis & Pipeline\n",
    "\n",
    "This extension adds the following sections:\n",
    "\n",
    "1. Advanced EDA (wordcloud, class balance plots, length distributions)\n",
    "2. Cross-validation and more robust model evaluation\n",
    "3. Hyperparameter tuning using GridSearchCV for LogisticRegression & RandomForest\n",
    "4. Error analysis â€” show misclassified examples and analyze common mistakes\n",
    "5. Build and save an sklearn `Pipeline` containing preprocessing + vectorizer + model\n",
    "\n",
    "Run the cells in order after running the earlier notebook cells (or run the whole notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ed480e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Advanced EDA\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 1) Review length distribution\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_length\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview length stats:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_length\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Advanced EDA\n",
    "# -------------------------\n",
    "# 1) Review length distribution\n",
    "df['review_length'] = df['review_text'].str.len()\n",
    "print('Review length stats:')\n",
    "print(df['review_length'].describe())\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(df['review_length'], bins=30)\n",
    "plt.title('Review length distribution')\n",
    "plt.xlabel('characters'); plt.ylabel('frequency')\n",
    "plt.show()\n",
    "\n",
    "# 2) Class balance percentage\n",
    "prop = df['sentiment'].value_counts(normalize=True) * 100\n",
    "print('\\nClass balance (%):\\n', prop)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.pie(prop.values, labels=prop.index, autopct='%1.1f%%')\n",
    "plt.title('Sentiment share (%)')\n",
    "plt.show()\n",
    "\n",
    "# 3) Optional: WordCloud (if wordcloud package installed)\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    all_text = ' '.join(df['text_clean'].values)\n",
    "    wc = WordCloud(width=800, height=400).generate(all_text)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud (all reviews)')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('wordcloud not available or failed to generate â€” skip. (Install wordcloud for nicer visuals)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42f351e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m skf \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      6\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      8\u001b[0m     scores \u001b[38;5;241m=\u001b[39m cross_val_score(model, X_vec, y_enc, cv\u001b[38;5;241m=\u001b[39mskf, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m     cv_results[name] \u001b[38;5;241m=\u001b[39m scores\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Cross-validation: evaluate models using cross_val_score (3-fold)\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "cv_results = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_vec, y_enc, cv=skf, scoring='f1_macro', n_jobs=-1)\n",
    "    cv_results[name] = scores\n",
    "    print(f'{name}  f1_macro scores: {scores}  mean: {scores.mean():.4f}  std: {scores.std():.4f}')\n",
    "\n",
    "# Visualize CV results\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.boxplot([cv_results[n] for n in cv_results.keys()], labels=list(cv_results.keys()))\n",
    "plt.title('Cross-validation f1_macro distribution')\n",
    "plt.ylabel('f1_macro')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c8b06f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m      7\u001b[0m param_grid_lr \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpenalty\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     12\u001b[0m gs_lr \u001b[38;5;241m=\u001b[39m GridSearchCV(LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m), param_grid_lr, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m gs_lr\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogisticRegression best params:\u001b[39m\u001b[38;5;124m'\u001b[39m, gs_lr\u001b[38;5;241m.\u001b[39mbest_params_, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest f1_macro:\u001b[39m\u001b[38;5;124m'\u001b[39m, gs_lr\u001b[38;5;241m.\u001b[39mbest_score_)\n\u001b[0;32m     15\u001b[0m tuned_models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogisticRegression\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m gs_lr\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Hyperparameter tuning (GridSearchCV) for LogisticRegression and RandomForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "tuned_models = {}\n",
    "\n",
    "# Logistic Regression grid\n",
    "param_grid_lr = {\n",
    "    'C':[0.01, 0.1, 1, 10],\n",
    "    'penalty':['l2'],\n",
    "    'solver':['lbfgs']\n",
    "}\n",
    "gs_lr = GridSearchCV(LogisticRegression(max_iter=2000), param_grid_lr, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "gs_lr.fit(X_train, y_train)\n",
    "print('LogisticRegression best params:', gs_lr.best_params_, 'best f1_macro:', gs_lr.best_score_)\n",
    "tuned_models['LogisticRegression'] = gs_lr.best_estimator_\n",
    "\n",
    "# Random Forest grid (keep it small to save time)\n",
    "param_grid_rf = {\n",
    "    'n_estimators':[50, 100],\n",
    "    'max_depth':[None, 20],\n",
    "    'min_samples_split':[2,5]\n",
    "}\n",
    "gs_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "gs_rf.fit(X_train, y_train)\n",
    "print('RandomForest best params:', gs_rf.best_params_, 'best f1_macro:', gs_rf.best_score_)\n",
    "tuned_models['RandomForest'] = gs_rf.best_estimator_\n",
    "\n",
    "# Evaluate tuned models on test set\n",
    "for name, model in tuned_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'\\n{name} on test set:')\n",
    "    print('accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "457a3ed4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrixDisplay\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Confusion matrix display for best_model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m y_pred_best \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      8\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_pred_best)\n\u001b[0;32m      9\u001b[0m disp \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(confusion_matrix\u001b[38;5;241m=\u001b[39mcm, display_labels\u001b[38;5;241m=\u001b[39mle\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Error analysis: show misclassified examples from the best baseline model (or tuned model if better)\n",
    "# We'll use the best_model (selected earlier). If tuned logistic/rf improved, you can replace best_model.\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Confusion matrix display for best_model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix (best_model)')\n",
    "plt.show()\n",
    "\n",
    "# Show some misclassified examples\n",
    "test_texts = np.array(df['text_clean'])[ (X_vec.shape[0] - X_test.shape[0]): ] if False else None\n",
    "# Instead, we'll reconstruct test indices using train_test_split again to match original split\n",
    "# Recreate indices to find which rows were in X_test\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in sss.split(X_vec, y_enc):\n",
    "    pass\n",
    "\n",
    "test_texts_orig = df.iloc[test_idx]['review_text'].values\n",
    "test_texts_clean = df.iloc[test_idx]['text_clean'].values\n",
    "y_test_orig = y_enc[test_idx]\n",
    "y_pred_orig = best_model.predict(X_vec[test_idx])\n",
    "\n",
    "# Find indices where predicted != true\n",
    "mis_idx = (y_pred_orig != y_test_orig).nonzero()[0]\n",
    "print(f'Misclassified count: {len(mis_idx)} (showing up to 20 examples)')\n",
    "for i in mis_idx[:20]:\n",
    "    print('\\n--- Example ---')\n",
    "    print('Original review:', test_texts_orig[i])\n",
    "    print('Cleaned review:', test_texts_clean[i])\n",
    "    print('True label:', le.inverse_transform([y_test_orig[i]])[0])\n",
    "    print('Predicted:', le.inverse_transform([y_pred_orig[i]])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Create sklearn Pipeline (preprocessing (identity) + vectorizer + model)\n",
    "from sklearn.pipeline import Pipeline\n",
    "# We'll include a simple identity transformer for text since preprocessing uses our function\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return [preprocess_text(x) for x in X]\n",
    "\n",
    "# Build pipeline using the tuned logistic regression (if exists) else best_model\n",
    "chosen_model = tuned_models.get('LogisticRegression', best_model)\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', TextPreprocessor()),\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words='english')),\n",
    "    ('clf', chosen_model)\n",
    "])\n",
    "\n",
    "# Fit pipeline on full data for final artifact\n",
    "pipeline.fit(df['review_text'].values, y_enc)\n",
    "# Save pipeline\n",
    "pipeline_path = os.path.join(MODEL_DIR, 'sentiment_pipeline.pkl')\n",
    "joblib.dump(pipeline, pipeline_path)\n",
    "print('Saved sklearn Pipeline to:', pipeline_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0ed70",
   "metadata": {},
   "source": [
    "### Final notes\n",
    "\n",
    "- This extended notebook added advanced EDA, cross-validation, hyperparameter tuning, error analysis, and an sklearn `Pipeline` saved as `sentiment_pipeline.pkl`.\n",
    "- After running, you can upload the extended notebook to GitHub and include the model files in a `models/` folder (or link to a cloud storage if file size is large).\n",
    "- If you want, I can also create a short `README.md` describing the project (problem statement, dataset, approach, results) suitable for GitHub. Would you like that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a15a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
